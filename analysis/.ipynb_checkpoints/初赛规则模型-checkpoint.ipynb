{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 360初赛规则模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import *\n",
    "from data import get_test_final_data, get_train_final_data\n",
    "from jieba.analyse import tfidf,textrank\n",
    "from joblib import Parallel,delayed\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = get_train_final_data()\n",
    "test_data = get_test_final_data()\n",
    "train_data.fillna(\"\",inplace=True)\n",
    "test_data.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyParallel(dfgroup, func, n_thread):\n",
    "    with Parallel(n_jobs=n_thread) as parallel:\n",
    "        res = parallel(delayed(func)(v) for k,v in dfgroup)\n",
    "        return pd.concat(res, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"content_last_str\"] = train_data.content.map(lambda x: x[-1] if len(x)>0 else \"\")\n",
    "test_data[\"content_last_str\"] = test_data.content.map(lambda x: x[-1] if len(x)>0 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_sent_count = dict()\n",
    "for content in train_data.content:\n",
    "    for sent in set(re.split('[' + ''.join([u'，', u'、', u'。', ',', '.', '!', '?', u'：', ':']) + ']', content)):\n",
    "        if len(sent) < 8:\n",
    "            continue\n",
    "        train_test_sent_count.setdefault(sent, 0)\n",
    "        train_test_sent_count[sent] += 1       \n",
    "for content in test_data.content:\n",
    "    for sent in set(re.split('[' + ''.join([u'，', u'、', u'。', ',', '.', '!', '?', u'：', ':']) + ']', content)):\n",
    "        if len(sent) < 8:\n",
    "            continue\n",
    "        train_test_sent_count.setdefault(sent, 0)\n",
    "        train_test_sent_count[sent] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(row):\n",
    "    content = row.content\n",
    "    sent_count = dict()\n",
    "    for sent in re.split('[' + ''.join([u'，', u'、', u'。', ',', '.', '!', '?', u'：', ':']) +']', content):\n",
    "        if len(sent) < 8:\n",
    "            continue\n",
    "        sent_count.setdefault(sent,0)\n",
    "        sent_count[sent] += 1\n",
    "    if sent_count:\n",
    "        values = sent_count.values()\n",
    "        sens = sent_count.keys()\n",
    "        \n",
    "        self_sens = []\n",
    "        common_sens = []\n",
    "        for sen in sens:\n",
    "            if train_test_sent_count[sen] > 1:\n",
    "                common_sens.append(sen)\n",
    "            else:\n",
    "                self_sens.append(sen)\n",
    "        self_sen_str = \"\".join(self_sens)\n",
    "        common_sen_str = \"\".join(common_sens)\n",
    "        self_sen_keywords = set(tfidf(self_sen_str))\n",
    "        common_sen_keywords = set(tfidf(common_sen_str))\n",
    "        row[\"common_keywords\"] = len(self_sen_keywords) + len(common_sen_keywords) - len(self_sen_keywords.union(common_sen_keywords))\n",
    "        \n",
    "        sen_in_others_count = list(map(lambda sen: train_test_sent_count[sen] ,sens))\n",
    "        row['sen_in_others_count_max'] = np.max(sen_in_others_count)\n",
    "        row['sen_in_others_count_q8'] =  pd.Series(sen_in_others_count).quantile(0.8)\n",
    "        row['sen_in_others_duplicated_num'] = np.sum(np.array(sen_in_others_count)>1)\n",
    "\n",
    "        row['in_content_duplicated_num'] = np.sum(np.array(list(values))>1)\n",
    "        row[\"max_len\"] = np.max(list(values))\n",
    "        row[\"sen_num\"] = len(values)\n",
    "        row['q8_sen_num'] = pd.Series(list(values)).quantile(0.8)\n",
    "    else:    \n",
    "        row[\"common_keywords\"] = 0\n",
    "        row['sen_in_others_count_max'] = 0\n",
    "        row['sen_in_others_count_q8'] =  0\n",
    "        row['sen_in_others_duplicated_num'] = 0\n",
    "        \n",
    "        row['in_content_duplicated_num'] = 0\n",
    "        row[\"max_len\"] = 0\n",
    "        row[\"sen_num\"] = 0\n",
    "        row['q8_sen_num'] = 0\n",
    "    row['if_，'] = row.content_last_str == u'，'\n",
    "    row['if_。'] = row.content_last_str == u'。'\n",
    "    row['if_、'] = row.content_last_str == u'、'\n",
    "    row['if_!']  = row.content_last_str == u'!'\n",
    "    row['if_?'] = row.content_last_str == u'?'\n",
    "    row['if_;'] = row.content_last_str == u';'\n",
    "    return row.to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = applyParallel(train_data.iterrows(), get_feature, 10)\n",
    "test = applyParallel(test_data.iterrows(), get_feature, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterfeats = [\"id\", \"title\",\"content\", \"label\", \"content_last_str\"]\n",
    "predictors = [feat for feat in train.columns if feat not in filterfeats]\n",
    "training_label = train_data.label.map(label2int).values\n",
    "training_data = train[predictors]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(training_data.astype(float), training_label, test_size= 0.1, random_state=0)\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "        'boosting_type': 'gbrt',\n",
    "        'objective': 'binary',\n",
    "        'num_leaves': 15,\n",
    "        'metric': [\"auc\"],\n",
    "        'learning_rate': 0.06,\n",
    "        'feature_fraction': 0.85,\n",
    "        'bagging_fraction': 0.85,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': 1,\n",
    "        'num_threads': 20,\n",
    "        'min_data_in_leaf': 100\n",
    "}\n",
    "gbm = lgb.train( params,\n",
    "                lgb_train,\n",
    "                num_boost_round=1000,\n",
    "                valid_sets=[lgb_train, lgb_eval],\n",
    "                verbose_eval = True,\n",
    "                early_stopping_rounds=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = gbm.predict(X_test)\n",
    "print(f1_score((pred>0.5).astype(int),y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BadCase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"proba\"] = gbm.predict(train[predictors].astype(float)) \n",
    "train[\"pred\"] = (train[\"proba\"]>0.5).astype(\"int\")\n",
    "train[\"label\"] = train_data.label.map(label2int)\n",
    "error = train[train.pred != train.label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error[error.label == 0].shape,error[error.label == 1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = error.sample(2)\n",
    "for i in n.iterrows():\n",
    "    print(i[1].title)\n",
    "    print(i[1].content)\n",
    "    print(int2label[i[1].label],i[1].proba,i[1].sen_num,i[1].sen_in_others_count_max,i[1].in_content_duplicated_num,\n",
    "         i[1].sen_in_others_duplicated_num,i[1].max_len,i[1].common_keywords)\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm.save_model(\"./gbm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, importances = zip(*(sorted(zip(gbm.feature_name(), gbm.feature_importance()), key=lambda x: x[1])))\n",
    "for name, importance in zip(names, importances):\n",
    "    print (name, importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['label'] = gbm.predict(test[predictors].astype(float),  num_iteration=gbm.best_iteration)\n",
    "test['label'] = (test['label'] > 0.5).astype(int).map(int2label)\n",
    "test_submit = test[['id', 'label']]\n",
    "test_submit.to_csv(Config.data_dir + '/submission.csv',index=None, header=None, encoding='utf8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
